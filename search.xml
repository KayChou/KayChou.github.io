<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[H.265/HEVC学习笔记]]></title>
    <url>%2F2019%2F10%2F15%2FH-265-HEVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[《新一代高效视频编码H.265HEVC》学习笔记 1.视频编码标准1.1 制定视频编码标准的两大组织： ITU-T：（International Telecommunication Union-Telecommunication Standardization Sector）国际电信联盟电信标准化部门 ISO/IEC：（International Organization for Standardization）国际标准化组织与（International Electrotechnical Commission）国际电工委员会 1.2 H.26X系列标准1.2.1 H.261标准： 于1990年由ITU-T制定，设计目的是为了在带宽为64kbits/s的倍数的综合业务数字网（Integrated Services Digital Network，ISDN）上传输质量可接受的视频信号，所以被称为p×64 bits/s编码器。 采用的编码方法包括基于运动补偿的帧间预测，DCT，量化，zig-zag扫描和熵编码等。 1.2.2 H.263标准 由ITU-T制定，仍以混合编码框架为核心，原始组织和码流组织与H.261十分相似。同时，H.263也吸收了一些MPEG等一些其他国际标准的技术，如半像素精度的运动估计，PB帧预测，无限制运动矢量和8×8的帧间预测。 H.263后来发展出两个增强版本：H.263+和H.263++。 1.2.3 H.264标准 由ITU-T的VCEG和ISO/IEC的MPEG组成的联合视频组（JVT）共同开发。也称H.264/AVC。 H.264/AVC仍然沿用了混合编码的概念，在此基础上支持了许多先进编码的技术，并获得了远超以往标准的编码性能。在相同重建质量的条件下，H.264/AVC比H.263+减少了50%的码率。 1.3 MPEG系列标准1.3.1 MPEG-1标准是由MPEG指定的第一个视频和音频有损压缩的标准，原本主要目标是针对数字存储媒体（CD光盘），后来成为VCD的核心技术。 1.3.2 MPEG-2标准于1994年面世，应用范围包括卫星电视，有线电视等，经过少量修改后成为DVD的主要技术。MPEG-2视频编码标准由MPEG与ITU-T联合制定，与H.262完全相同。 1.3.3 MPEG-4标准于1998年被ISO/IEC批准，相比于MPEG-1和MPEG-2，MPEG-4涵盖的内容非常丰富。包括多达31个部分。MPEG-4分别定义了系统，音视频编码，多媒体传输集成框架等，其中第10部分就是H.264/AVC。 1.4 H.265/HEVC简介2010年4月VCEG和MPEG再次组建视频编码联合组（JCT-VC），联手制定H.265/HEVC（High Efficiency Video Coding）。从根本上说，H.265/HEVC视频编码标准的编码框架并没有革命性的改变，仍采用混合编码框架，包括变换，量化，熵编码，帧内预测，帧间预测以及环路滤波等模块，但在几乎每个模块都引入了新的编码技术。（1）帧内预测：去除空间冗余。（2） 帧间预测：去除时间冗余。（3）变换量化：通过对残差数据进行变换量化以去除频域相关性，对数据进行有损压缩。将图形变换至频域，将能量集中在低频区域。（4）去方块滤波：基于块的视频编码形成的重构图像会出现方块效应，采用方块滤波可以削弱甚至消除方块效应。（5）样点自适应补偿：解析去方块滤波后的像素的统计特性，为像素添加相应的偏移值，在一定程度上削弱振铃效应。（6）熵编码：将编码控制数据，量化变化系数，帧内预测数据以及运动数据编码为二进制流进行存储或传输。熵编码模块的输出即为原始视频压缩后的码流。相对于以往的视频编码标准，H.264/HEVC的编码性能有了很大的提升，这源于新编码工具的使用和自身独居特色的核心技术。例如基于四叉树的灵活块分割结构，不同角度的帧内预测模式，自适应的运动矢量预测，合并技术Merge，可变尺寸的DCT，模式依赖的DST和性能更好的CABAC，以及新的样点自适应补偿滤波器等。（7）编码单元：H.264/AVC标准中的核心编码单元是宏块，包含一个16×16的亮度块采样，对于一般的视频信源，会伴随两个8×8的色度块采样。而H.265/HEVC采用了编码树单元和编码树块，大小可以由编码器设定，并且可以超越16×16。（8）改进的帧内预测技术：H.264/AVC对4×4的编码块采用9种预测模式，对16×16的编码块采用4种预测模式。H.265/HEVC提供了35种帧内预测模式。（9）先进的帧间预测技术：H.265/HEVC中引入了新的帧间预测技术，包括运动信息融合技术，先进的运动矢量预测技术以及基于Merge的Skip模式。（10）RQT（Residual Quad-tree Transform）技术是基于四叉树结构的自适应变换技术。（11）ACS（Adaptive Cofficient Scanning）技术包括三类：对角扫描，水平扫描和垂直扫描。（12）SAO像素自适应补偿技术。位于去块效率滤波器之后，用于补偿重构像素值，达到减少振铃效应失真的目的。（13） IBDI（Internal Bit Depth Increase）技术，在编码器的输入端将未压缩图像的像素深度由P比特增加到Q比特，在解码器输出端再恢复到P比特。 2 颜色空间2.1 RGB2.2 YUV主要用于优化彩色视频信号的传输，并使其向后兼容老式黑白电视。其中Y表示明亮度，U和V表示色度。色度U反映的是RGB信号蓝色部分与亮度值之间的差异，V反映的是RGB信号红色部分与信号亮度值之间的差异。 2.3 YCbCr：与YUV类似，Y表示明亮度。（1）4:4:4：每4个亮度样本都对应4个Cb和4个Cr色度样本。（2） 4:2:2：每2个亮度样本都对应1个Cb和1个Cr色度样本。（3） 4:1:1：水平方向上每4个亮度样本都对应1个Cb和1个Cr色度样本。垂直方向分辨率相同。（4）4:2:0：水平和垂直方向上每4个亮度样本都对应1个Cb和1个Cr色度样本。 3 H.265/HEVC编码视频格式不同格式的源视频通过前处理模块转换成统一的数据格式，经过编解码器后再转换成源视频格式。 3.1 编码图像格式主要包括矩阵数量及空间关系，图像空间分辨率，像素两化深度等。承载编码图像格式的语法元素属于SPS（序列参数集），SPS表征一组图像CVS（编码视频序列）的共有参数。 3.2 解码图像格式除包含图像携带的格式信息，还包括扫描类型、图像类型、色彩空间等信息。 4 编码结构视频序列由若干时间连续的图像构成，在压缩之前，先将视频序列分割为若干个小的图像组（GOP）。GOP又分为封闭式GOP和开放式GOP。封闭式GOP以IDR（Instantaneous Decoding Refresh）图像开始，各个GOP之间独立编解码。开放式GOP，第一个GOP中的第一个帧内编码图像为IDR图像，后面GOP中的帧内编码图像可以越过non-IDR图像使用前一个GOP的已编码图像做参考图像。每个GOP又分为很多片（slice），片与片之间进行独立编解码，每个片由一个或多个片段（Slice Segment，SS）组成。此外，H.265/HEVC又引入了树形结构单元（CTU）。每个CTU包括一个亮度树形编码块（CTB）和两个色差树形编码块。一个SS在编码时，先被分割成大小相同的CTU，每一个CTU按照四叉树分割方式被划分为不同类型的编码单元（Coding Unit，CU）。在码流结构方面，H.265/HEVC将属于GOP，Slice层中共用的大部分语法元素游离出来，组成序列参数集（SPS）和图像参数集（PPS）。SPS大致包括解码相关信息，如档次级别，分辨率，时域可分级信息等。PPS包含了一幅图像所用的公共参数，即一幅图像中的所有SS引用同一个PPS。大致内容包括初始图像控制信息，如初始量化参数（QP）、分块信息等。此外，为了兼容标准在其他应用上的拓展，H.265/HEVC的语法架构增加了视频参数集（VPS）。其内容大致包括多个子层共享的语法元素。对于一个SS，通过引用它的PPS，该PPS又引用其对应的SPS，该SPS再引用它所对应的VPS，最终得到SS的公用信息。 4.1 视频参数集（VPS）主要用于传输视频分级信息，一个给定的视频序列，无论其每一层的SPS是否相同，都参考相同的VPS。VPS包含的信息主要有：多个子层和操作点共享的语法元素；会话所需要的有关操作点的关键信息，如档次、级别；其他不属于SPS的操作点特性信息。 4.2 序列参数集（SPS）对于一段视频码流，其可能包含一个或者多个编码视频序列CVS。SPS的作用就是包含一个CVS中所有编码图像的共享编码参数，SPS通过被PPS引用而作用于编码图像。一个CVS中所有使用的PPS都引用同一个SPS。内容主要包括：1）图像格式信息，如采样格式，图像分辨率，量化深度，裁剪参数等；2）编码参数信息，包括编码块、变换块的最大最小尺寸等；3）与参考图像相关的信息，包括短期参考图像的设置，长期参考图像的使用和数目；4）档次，层和级相关参数；5）时域分集信息，包括时域子层的最大数目，控制传输POC仅为参数；6）可视化可用信息；7）其他信息如当前SPS引用的VPS编好，SPS标识号和SPS扩展信息。 4.3 图像参数集（PPS）在编码视频流中，一个CVS包含多幅图像，每幅图像可能包括一个或多个SS，每个SS提供了其所引用的PPS标识号，依次得到相应PPS中的共用信息。对于同一幅图像，其内所有SS都用一个PPS。PPS的主要内容有：1）编码工具的可用性标志，编码工具主要包括符号位隐藏，帧内预测受限，去方块滤波等；2）量化过程相关句法元素；3）Tile相关句法元素；4）去方块滤波相关句法元素；5）片头控制信息；6）其他编码一幅图像可以共用的信息。 4.4 片段层（SS）一幅图像可以被分割为一个或多个片（Slice），每个片的压缩数据都是独立的。Slice不能跨过边界来进行帧内或帧间预测，但允许环路滤波器跨过边界进行滤波。使用Slice的目的是为了当数据丢失后能在此保证解码同步。根据编码类型不同，Slice可以分为：（1）I Slice：该Slice的所有CU的编码过程都使用帧内预测。（2）P Slice：P帧中的CU可以使用帧间预测，每个预测块（PB）使用至多一个运动补偿预测信息。（3）B Slice：B帧的CU也可以使用帧间预测，但是每个PB可以使用至多两个运动补偿预测信息。一个独立的Slice可以进一步划分为若干SS，包括一个独立SS和若干个依赖SS，并且以独立SS作为该Slice的开始。独立SS是指它所涉及的句法元素可以由自身确定，依赖SS是指它所涉及的某些句法元素由已解码的独立SS推导得到。一个SS包含整数个CTU，并且这些CTU分布在同一个NAL单元中。H.265/HEVC编码的最高层为SS层，SS层所需要的图像层信息可以通过引用相应的PPS来获得。SS头包含其引用的PPS标识号，同一幅图像中的所有SS引用同一个PPS。 4.5 Tile单元H.265/HEVC相对于H.264/AVC的改进之处还在于Tile的提出。一幅图像不仅可以划分为若干个Slice，也可以划分为若干个Tile。即从水平和数值方向将一幅图像分割为若干个矩形区域，每个矩形区域都是一个Tile。每个Tile包含整数个CTU，可以独立解码。Tile提供比CTB更大程度上的并行。通常情况下，每个Tile中包含的CTU的数据是近似相等的。在一幅图像中，可以同时存在某些Slice包含多个Tile和某些Tile中包含多个Slice的情况。 4.6 Slice与TileSlice与Tile划分的目的都是为了进行独立解码，但二者划分方式有所不同。Tile形状基本上为矩形，Slice则为条带状。Slice由一系列的SS组成，一个SS由一系列的CTU组成。Tile则直接由一系列的CTU组成。每个Slice/SS和Tile至少要满足一下两个条件之一：1）一个Slice/SS中的所有CTU属于同一个Tile；2）一个Tile中的所有CTU属于同一个Slice/SS。 4.7 树形编码块（CTU）传统的视频编码都是基于宏块实现的。H.265/HEVC标准中引入了树形编码单元CTU，其尺寸由编码器指定，可以大于宏块尺寸。为了灵活高效地表示视频场景中的不同纹理细节、运动变化的视频内容或者视频对象，H.265/HEVC为图像划分定义了一套全新的语法单元，包括编码单元（CU）、预测单元（PU）和变换单元（TU）。 编码单元CU在H.264/AVC中，编码块CB的大小是固定的，而在H.265/HEVC中，一个CTB可以直接作为一个CB，也可以进一步以四叉树的形式划分为多个小的CB。亮度CB最大为64×64，最小为8×8。一个亮度CB和相应的色度CB及它们相关的句法元素共同组成一个编码单元CU。 预测单元PU一切与预测有关的信息都定义在预测单元部分，比如帧内预测的方向、帧间预测的分割方式、运动矢量预测以及帧间预测参考图像索引号等都属于PU的范畴。对于一个2N×2N的CU模式，帧内预测单元PU的可选模式有两种，帧间预测单元PU的可选模式有8种：4种对称模式和4种非对称模式。如下图所示。此外还有skip模式，skip模式是帧间预测的一种，当需要编码的运动信息只有运动参数集索引，编码残差信息不需要编码时，为2N×2N skip模式。 变换单元TU是独立完成变换和量化的基本单元。H.265/HEVC突破了原有的变换尺寸限制，可支持大小为4×4~32×32的编码变换。TU的大小依赖于CU，在一个CU内，以四叉树的形式递归划分TU。 4.8 档次、层和级别H.264中已经有对档次（profile）和级别（level）的划分，而H.265/HEVC在此基础上又定义了新的概念：层（Tier）。档次主要规定编码器可以采用哪些编码工具或算法，级别则根据解码器负载和存储空间情况对关键参数加以限制。考虑到应用可以根据最大码率和CPB（解码缓冲区）大小来区分，因此有些Level定义了两个Tier：主层（Main Tier）和高层（High Tier）。主层用于大多数应用，高层用于最苛刻的应用。档次：H.265/HEVC中提出了三种档次：Main, Main 10和Main Still Picture。限制条件如下：1）只支持4:2:0色度采样信号；2）使用了Tiles便不能使用WPP，每一个Tile的亮度分辨率至少要256×64；3）Main和Main Still Picture档次支持8位像素深度，Main 10档次则支持10位像素深度，Main Still Picture档次不支持帧间预测。层和级别：H.265/HEVC定义了两个层和13个级，两个层分别为Main Tier和High Tier。4和4以上的8个Level支持High Tier。 5预测编码对于视频信号来说，一幅图像的邻近像素之间有着较强的空间相关性，相邻图像之间有很强的时间相关性。采用帧内预测和帧间预测的方式，有效去除视频空域和时域的相关性，编码器对预测后的残差而不是原始像素值进行变换、量化、熵编码由此大幅提高编码效率。 5.1 预测编码原理预测编码是指利用已编码的一个或几个样本值，根据某种模型或方法，对当前的样本值进行预测，并对样本真实值和预测值之间的差值进行编码。联合编码和条件编码时两种有记忆信源的有效编码方式。联合编码通常将图像分割成固定的块，将每一个块作为一个信源符号来考察，对每一个块内的像素进行联合编码。联合编码充分利用一个块内像素间的相关性，但未能利用相邻块之间的相关性。条件编码中当前像素的编码依赖于邻近已邻近编码像素，各像素将以滑动窗口的形式进行条件编码，这种方式改善了联合编码的缺陷，图像内邻近像素之间的相关性得到了充分利用。预测编码技术通过预测模型消除橡塑件的相关性，得到的差值信号可以认为没有相关性或相关性很小，因此可以作为无记忆信源进行编码。预测编码的基本过程如下图所示。 视频预测编码技术主要分为两大类：帧内预测和帧间预测。 5.1.1 帧内预测编码Harrison首先在图像编码中研究了帧内预测方法，方法师用先前已编码的像素进行加权平均作为当前像素的预测值。这一基本思想最终被应用于JPEG-LS标准的LOCO-I算法中。后来随着DCT在图像、视频编码中的广泛应用，帧内预测转为在频域进行，如JPEG，H.261，MPEG-I，MPEG-2和H.263等。H.264/AVC标准规定了若干种预测模式，每一种模式都对应一种纹理方向，当前块预测像素由其预测方向上相邻块的边界重建像素生成。为了选择最合适的帧内预测模式，H.264/AVC使用拉格朗日率失真优化（RDO）进行模式选择。H.264/AVC标准以及后来的FRExt扩展层一共规定了3种大小的亮度帧内预测模块：4×4、8×8和16×16。其中4×4、8×8块包含9种预测模式，16×16块包含4种预测模式。色度分量的帧内帧内预测都是基于8×8大小的块进行的，也有4种预测模式。（1）亮度分量的帧内16×16模式帧内16×16模式包含4种预测模式：垂直模式、水平模式、DC模式和Plane模式，如下图所示。垂直模式：当前块预测像素由上方相邻块重建像素产生；水平模式：当前块预测像素由左侧相邻块重建像素产生；DC模式：当前块预测像素都为其所有参考像素的平均值；Plane模式。 （2）亮度分量帧内4×4模式帧内4×4和8×8块都包含9种预测模式，且两者方法类似。以帧内4×4的9种预测模式为例，包含了8种不同的预测方向以及DC模式：1）垂直模式；2）水平模式；3）DC模式；4）左下对角线模式；5）右下对角线模式；6）垂直向右模式；7）水平向下模式；8）垂直向左模式；9）水平向上模式。（3）8×8色度帧内预测模式色度帧内预测包含4中模式：DC模式、水平模式、垂直模式和Plane模式。最新的H.265/HEVC标准对其进行了进一步发展。一方面，H.265/HEVC使用了更多大小的预测块，以适应高清视频的内容特征；另一方面，H.265/HEVC规定了更多种预测模式，对应于更多种不同的预测方向，以适应更加丰富的纹理。 5.1.2 帧间预测编码由于视频序列通常包括较强的时域相关性，因此预测残差通常是“平坦的”，即很多残差值接近于0。将残差信号作为后续模块的输入进行变换、量化、扫描及熵编码，可实现对视频信号的高效压缩。目前主要的视频编码标准帧间预测都采用了基于块的运动补偿技术。主要原理是为当前图像的每个像素块在之前已编码图像中寻找一个最佳匹配块，该过程称为运动估计。其中用于预测的图像称为参考图像，参考块到当前像素块的唯一称为运动向量，当前像素块与参考快的差值称为预测残差。早期的H.261定义了两种类型的图像——I图像和P图像。其中I只能使用帧内编码，而P则可以利用帧间预测编码。此外，为了去除相邻块运动向量之间的相关性，H.261对MV进行了差分编码。在H.261中，P图像的预测方式必须是由前一幅图像预测当前图像，这称为“前向预测”，但实际场景旺旺会产生不可不预测的运动和遮挡，因此当前图像的某些像素块可能无法从之前的图像中找到匹配块。为此，MPEG-1标准中定义了第三类图像——B图像，并规定B可以使用三种预测方式：前向预测、后向预测以及双向预测。这样，B中的一个宏块可以对应两个MV：一个由前向预测得来，另一个由后向预测得来。此外，MPEG-1首次使用了半像素精度的运动估计，其半像素位置的参考像素值可由双线性差值方法产生。面向数字广播电视的标准MPEG-2首次支持了隔行扫描视频，一帧图像包含两个场——顶场和底场。为了适应这种情况，每个帧图像的宏块需要被拆分为两个16×8的块分别进行预测。H.263标准沿用了MPEG-1的双向预测与半像素精度运动估计，并进一步发展了MPEG-2中将一个宏块分成更小的块进行预测的思想。此外，H.263改进了MV的预测机制——用当前块左方、上方及右上方的3个MV的中值来预测当前块的MV。H.264/AVC标准在集成以往标准帧间预测成熟技术和框架的同时，对其进行了进一步细化和改善。为了提高运动补偿的精度，H.264/AVC规定了7种大小的运动补偿块，并且一个宏块内部允许存在不同大小块的组合。编码器可以根据视频内容自适应的选择块大小。此外还使用了1/4像素精度运动估计（色度为1/8像素精度）、多参考图像预测、加权预测以及空域/时域MV预测等。 5.1.3 帧间预测编码关键技术（1） 运动估计运动估计准则：1）最小均方误差MSE准则；2）绝对误差和SAD准则；3）最大匹配像素数MPC准则。搜索算法：常用的搜索算法有：全搜索算法，二维对数走索，三步搜索算法等。亚像素精度估计：亚像素精度运动估计意味着需要对参考图像进行插值，好的插值方法能够大幅改善运动补偿的性能。（2） MV预测在大多数图像和视频中，一个运动物体可能会覆盖多个运动补偿块，因此空间域相邻块的运动向量具有较强的相关性。若使用相邻已编码块对当前块MV进行预测，将二者差值进行进行编码，则会大幅节省编码MV所需的比特数。H.264/AVC使用了空域和时域两种MV的预测模式。（3） 多参考图像加权预测对于某些场景，如物理周期性变化，多参考图像可以大幅提高预测精度。早起视频编码标准只支持单个参考图像，H.263+开始支持多参考图像预测技术，而H.264/AVC标准最多支持15个参考图像。为了权衡编码效率与编码时间，一般情况下都采用4~6个参考图像。此外，H.264/AVC还使用了加权预测技术，加权预测表示预测像素可以用一个或两个参考图像中的像素与加权系数相乘得出。 5.2 H.265/HEVC帧内预测5.2.1 亮度帧内预测模式H.265/HEVC亮度分量帧内预测支持5中大小的PU：4×4，8×8，16×16，32×32和64×64，其中每一种大小的PU都对应35种预测模式，包括Planar模式，DC模式以及33种角度模式。所有的预测模式都是用相同的模板。 （1）Planar模式：由H.264/AVC中的Plane模式发展而来，它适用于像素值缓慢变化的区域，Planar模式使用水平和垂直方向的两个线性滤波器，并将二者的平均值作为当前块像素的预测值。（2）DC模式：适用于大面积平坦区域，其做法与H.264/AVC基本相同。当前块预测值可由其左侧和上方参考像素的平均值得到。 （3）角度模式：相比于H.264/AVC使用了8种不同的预测方向想，H.265/HEVC则进一步细化了这些预测方向，规定了33中角度预测模式。上图给出了这33种预测角度模式。 5.2.2 亮度模式编码H.265/HEVC建立了一个帧内预测模式候选列表candModeList，表中有三个候选预测模式，用于存储相邻PU的预测模式。 5.2.3 色度模式编码H.265/HEVC色度分量帧内预测一共有5种模式：Planar模式，垂直模式，水平模式，DC模式以及对应亮度分量的预测模式。 5.2.4 帧内预测过程在H.265/HEVC中，35种预测模式是在PU的基础上定义的，而具体帧内预测过程的实现则是以TU为单位的。标准规定PU可以以四叉树的形式划分TU，且一个PU内的所有TU共享同一种预测模式。H.265/HEVC帧内预测可以分为3个步骤：1）判断当前TU相邻参考像素是否可用并做相应处理；2）对参考像素进行滤波；3）根据滤波后的参考像素计算当前TU的预测像素值。（1）相邻参考像素的获取对于一个N×N的TU，参考像素区域可分为5个部分：左下，左侧，左上，上方和右上，一共4N+1个像素点。若该TU位于图像边界，或Slice、Tile的边界（H.265/HEVC规定帧内编码中相邻Slice和Tile不能相互参考），则相邻参考像素可能不存在或不可用。另外在某些情况下左下和右上区域尚未编码时，这些参考像素也是不可用的。当参考像素不存在或者不可用时，H.265/HEVC标准会使用最相邻的像素进行填充。若所有区域参考像素都不可用，则参考像素都用固定值填充。（2）参考像素的滤波H.264/AVC在帧内预测时对某些模式下的参考像素进行了滤波，以更好地利用临近像素之间的相关性，提高预测精度。H.265/HEVC沿用了这一做法。一方面，H.265/HEVC针对大小不同的TU选择了不同数量的模式进行滤波。DC模式以及4×4大小的TU都不需要进行参考像素滤波，32×32的TU除水平模式，垂直模式之外的所有角度模式以及Planar模式，其他都需要参考像素滤波，16×16TU进一步除去最接近水平和垂直方向的四个模式。8×8TU仅对3个45°倾斜方向的模式以及Planar模式进行参考像素滤波。另一方面，H.265/HEVC增加使用了一种强滤波方法。（3）预测像素的计算 5.3 H.265/HEVC帧间预测5.3.1 运动估计在基于块运动补偿的视频编码框架中，运动搜索是最为重要的环节之一，同时也是编码端最耗时的模块。H.265/HEVC官方测试编码器HM10.0给出了两种搜索算法：全搜索和TZSearch算法。TZSearch是H.265/HEVC新出现的技术，步骤为：1）确定其实搜索点：H.265/HEVC采用AMVP，AMVP给出若干个候选测试MV，编码器从中选择率失真代价最小的的作为预测MV。将其指向的位置作为起始搜索点。2）以步长1开始，按照菱形模板或正方形模板在搜索范围内进行搜索，其中步长以2的幂次递增，选出率失真代价最小的点作为该步骤的搜索结果。3）若上一步得到的最优点步长为1，则需要在该店周围做两点搜索，主要目的是补充搜索最优点周围尚未搜索的点。4）若最优点步长大于一定的阈值，则以该最优点为中心，在一定范围内做全搜索。5）以最优点为新的启示搜索点，重复2~4，细化搜索。TZSearch与全搜索算法相比性能上略有降低，但搜索时间仅为全搜索的一半。在实际场景中，由于物体运动距离不一定是像素的整数倍，因此需要将运动估计的精度提升到亚像素级别。H.265/HEVC沿用了上一代标准H.264/AVC所使用的1/4像素精度运动估计，并进一步发展了其亚像素差值算法，如亮度分量差值算法，色度分量插值算法等。 5.3.2 MV预测技术空域上相邻块的MV具有较强的相关性，同时MV在时域上也有一定的相关性。若利用空域或时域上相邻块的MV对当前MV进行预测，仅对残差进行编码，则能够大幅节省MV的编码比特数。H.265/HEVC在MV的预测方面提出了两种技术——Merge技术和AMVP技术。（1）Merge技术：Merge模式会为当前PU建立一个MV候选列表，列表中存在5个候选MV，通过遍历这5个候选MV，并进行率失真代价的计算，最终选取率失真代价最小的一个作为该Merge模式的最优MV。Merge模式建立的MV候选列表中包含了空域和时域两种情形。（2）AMVP技术：高级运动向量预测利用空域时域上运动向量的相关性为当前PU建立了候选MV列表，编码器从中选出最优的预测MV，并对MV进行差分编码，解码器通过建立相同的列表，仅需要运动向量残差（MVD）与预测MV在该列表中的序号即可计算当前PU的MV。 5.3.3 加权预测加权预测可以 用于修正P帧或B帧中的运动补偿预测像素，H.265/HEVC标准规定了两种加权预测方法——默认加权预测以及Explicit加权预测。 5.4 PCM模式H.265/HEVC中有一种特殊的编码模式——PCM模式。在该模式下，编码器直接传输一个CU的像素值，而不经过预测、变换等其他操作。对一些特殊的情况，如图像内容极不规则或量化参数QP非常小时，该模式与传统的“帧内—变换—量化—熵编码”相比编码效率可能更高，此外，PCM模式还适用于无损编码情形。 6 变换编码6.1 离散余弦变换傅里叶变换表明，任何信号都能表示为多个不同振幅和频率的正弦波或余弦波信号的叠加。对二维DCT基图像来说，其中左上角小图像表示水平和垂直空间频率均为零时的基图像，其余分别对应于不同水平和垂直空间频率的基图像。对于灰度值变化缓慢的像素块来说，经过DCT后绝大部分能量都集中在在左上角的低频系数中。实际上，大多数图像包含更多的低频分量，并且可以利用人眼对图像高频细节相对不敏感的特性，对高能量的低频系数进行较为精细的量化和处理，而对低能量的高频系数进行粗略的量化或掩盖，这样可以较好地压缩图像而不会造成明显的主观质量下降。随着变换尺寸的增大，DCT去相关性能越来越好，但提升幅度逐渐变缓。考虑到DCT的计算复杂度会随着变换尺寸的增大而大幅提高，因此许多较早的图像与食品编码标准都采用8×8 DCT作为其变换编码方案。 6.1.1 整数DCTDCT中余弦函数的存在使得DCT过程必须使用浮点数，这样不可避免地会带来舍入误差以及编解码正反变换失配的问题。针对该问题，从H.264/AVC开始采用整数DCT。H.265/HEVC沿用了H.264/AVC所采用的的整数DCT技术，但其变换矩阵与H.264/AVC相比有所不同。H.265/HEVC使用了4种不同尺寸的整数DCT。]]></content>
      <categories>
        <category>视频编码</category>
      </categories>
      <tags>
        <tag>H.265/HEVC</tag>
        <tag>视频编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于LSTM的2048游戏AI]]></title>
    <url>%2F2019%2F01%2F30%2F%E5%9F%BA%E4%BA%8ELSTM%E7%9A%842048%E6%B8%B8%E6%88%8FAI%2F</url>
    <content type="text"><![CDATA[基于LSTM的2048游戏AI。 本项目在duducheng的基础上，通过循环卷积神经网络（RNN）的变体——训练了一个模型，实现了一个2048游戏AI。实测该模型平均可以达到1300分以上。 点击链接查看项目源码 本文主要从以下几个方面说明该项目的方法和原理： 运行环境 数据集获取及定义 网络模型搭建 模型训练 结果测试 运行环境说明本项目运行环境为python3 + torch。此外，数据集处理及存储需要使用pandas库。 数据集获取及定义数据集主要从duducheng实现的基于决策实现的算法获取。这里我们称之其为“强Agent”。调用强Agent运行2048游戏，将当前棋盘的状态当做数据，强Agent的预测结果作为label。并且对棋盘数据进行取对数的预处理。存储格式如下图所示。其中每一行有17个数.前16个代表当前棋盘，最后一个为当前棋盘的预测结果。 数据集定义方式如下： 123456789101112131415161718192021222324class MyDataset(torch.utils.data.Dataset): def __init__(self, root, transform=None, target_transform=None): dataframe = pd.read_csv(root) data_array = dataframe.values self.data = data_array[:, 0:16] self.label = data_array[:, 16] self.transform = transform self.target_transform = target_transform def __getitem__(self, index): board = self.data[index].reshape((4, 4)) board = board[:, :, np.newaxis] board = board/11.0 # board = torch.from_numpy(board) label = self.label[index] if self.transform is not None: board = self.transform(board) return board, label def __len__(self): return len(self.label) 导入数据集方式如下。 12345678910111213141516171819202122def load_data(): train_data = MyDataset( root = './Datasets/Train.csv', transform=transforms.Compose( [transforms.ToTensor()])) train = torch.utils.data.DataLoader( train_data, batch_size=batch_size, shuffle=True, num_workers=0) test_data = MyDataset( root = './Datasets/Test.csv', transform=transforms.Compose( [transforms.ToTensor()])) test = torch.utils.data.DataLoader( test_data, batch_size=batch_size, shuffle=True, num_workers=0) return train, test 网络模型搭建1234567891011121314151617class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.RNN = nn.LSTM( input_size = 4, hidden_size = 300, num_layers = 4, batch_first=True) self.fc1 = nn.Linear(300, 64) self.fc2 = nn.Linear(64, 4)def forward(self, x): x, (h_n, h_c) = self.RNN(x, None) x = x[:, -1 ,:] x = self.fc1(x) x = self.fc2(x) return F.log_softmax(x, dim=1) 模型训练12345678910111213141516171819202122232425262728# Train the netdef train(model, epoch, train_loader, optimizer): model.train() for idx, (data, target) in enumerate(train_loader): data = data.type(torch.float) data = Variable(data.view(-1,4,4)) if torch.cuda.is_available(): data = Variable(data).cuda() target = Variable(target).cuda() model.cuda() output = model(data) optimizer.zero_grad() # target = target.repeat(12) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if idx % 10 == 0: predict = output.data.max(1)[1] num = predict.eq(target.data).sum() correct = 100.0*num/batch_size t = time.time()-start_time print('Train epoch: %d Loss: %.3f ' % (epoch+1, loss), \ 'Accuracy: %0.2f' % correct, '%', '\tTotal Time: %0.2f' % t) 测试结果]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Pytorch</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3配置anaconda编译环境]]></title>
    <url>%2F2018%2F10%2F07%2FSublime-Text-3%E9%85%8D%E7%BD%AEanaconda%E7%BC%96%E8%AF%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04下配置Sublime Text 3的anaconda编译环境。 默认的Sublime Text 3 编译系统中只有python编译，没有anaconda编译，但是很多情况下，我们总是希望能在sublime text 下支持anaconda编译。在已经安装好anaconda的前提下，配置方式如下所述。 打开sublime text 3,点击上部菜单栏Tools-&gt;Build System-&gt;new Build System,如下图所示。 点击后，会打开一个新的配置文件，在空白配置文件中拷贝以下代码。 12345&#123; "cmd": ["/home/benjamin/anaconda3/bin/python", "-u", "$file"], "file_regex": "^[ ]*File \"(...*?)\", line ([0-9]*)", "selector": "source.python" &#125; 其中，”/home/benjamin/anaconda3/bin/python”为anaconda所在的环境路径，需要读者自己修改为自己电脑上的环境。 保存配置文件，命名为anaconda。 至此，在sublime text3下的anaconda编译环境就配好了，可以在Tools-&gt;build System中进行选择。]]></content>
      <categories>
        <category>Ubuntu装机</category>
      </categories>
      <tags>
        <tag>Ubuntu16.04</tag>
        <tag>Sublime Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04配置shadowsocks-qt5客户端]]></title>
    <url>%2F2018%2F09%2F16%2FUbuntu16-04%E9%85%8D%E7%BD%AEshadowsocks-qt5%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04下配置shadowsocks客户端实现浏览器翻墙。 安装shadowsocks-qt5首先安装shadowsocks的图形化界面。在终端中依次输入以下三行代码： 123sudo add-apt-repository ppa:hzwhuang/ss-qt5sudo apt-get updatesudo apt-get install shadowsocks-qt5 在开始菜单中搜索shadowsocks-qt5，图标如下图所示，点击打开。 打开后，点击上方菜单栏Edit，在弹出框如图所示，在弹出框中依次填写相应的服务器IP地址和密码。 配置完成后，点击Connect，即可连接成功。 注意，虽然此时已经翻墙，但是Ubuntu此时不会实现全局代理。这是因为shadowsocks只能代理SOCKS5的流量，但Ubuntu走的是https的流量。因此，还需要浏览器搭配相应的插件才能实现翻墙。这里只讲最常用的浏览器chrome。 若Ubuntu中尚未安装chrome，可以到Ubuntu Chrome下载安装。 Chrome需要安装相应的插件，最常用的是SwitchyOmega。下载到本地后，将下载下来的crx文件拖动到Chrome浏览器中即可实现安装。安装完成后会自动弹出SwitchyOmega的配置界面。 点击New Profile，命名可以随便取，这里笔者命名为vultr（为VPN供应商的名称），里面内容按上图中填写即可。最后点击Apply changes退出即可。 在shadowsocks中点击connect连接成功后，在浏览器中右上角插件中找到SwitchyOmega，单击后弹出以下界面： 此时默认的是直接连接，不会走任何代理，在需要翻墙时，切换到相应的代理（笔者这里是vultr）即可实现翻墙。 后记欢迎大家在评论区指正和评论。有任何问题也可以在评论区提出。]]></content>
      <categories>
        <category>Ubuntu装机</category>
      </categories>
      <tags>
        <tag>Ubuntu16.04</tag>
        <tag>Shadowsocks GUI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下通过pip安装PyTorch导入报错]]></title>
    <url>%2F2018%2F09%2F07%2FWindows%E4%B8%8B%E9%80%9A%E8%BF%87pip%E5%AE%89%E8%A3%85PyTorch%E5%AF%BC%E5%85%A5%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[报错详情12345Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\Users\username\Code\Python\Test\venv_pytorch\lib\site-packages\torch\__init__.py", line 78, in &lt;module&gt; from torch._C import *ImportError: DLL load failed: 找不到指定的模块 解决办法该问题是由于Numpy和当前python版本不兼容造成的。在安装pytorch时，执行 1pip3 install torchvision 该安装命令会自动安装依赖包numpy。主动卸载安装的numpy 1pip3 uninstall numpy 点击访问非官方python拓展库，下拉找到numpy: 选择与自己电脑python对应版本相同的numpy包，点击下载。以笔者的安装环境为例：python版本为64位python35，则选择下载 numpy‑1.15.1+mkl‑cp35‑cp35m‑win_amd64.whl 。 打开下载文件的存储路径，在当前路径下，通过命令行执行： 1pip3 install numpy‑1.15.1+mkl‑cp35‑cp35m‑win_amd64.whl 等待安装完成，再次导入torch，成功导入！ 后记欢迎大家在评论区指正和评论。有任何问题也可以在评论区提出。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch入门之MNIST分类实例]]></title>
    <url>%2F2018%2F09%2F03%2FPytorch%E5%85%A5%E9%97%A8%E4%B9%8BMNIST%E5%88%86%E7%B1%BB%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[手写体数字识别，MNIST分类实例。 初学机器学习，尝试做了一个简单的手写数字识别。本实例选用的是MNIST数据集，基于卷积神经网络，通过两个卷积层，两个池化层和两个全连接层，实现了手写体数字识别。实际测试识别准确率达到98%。这里分享一下我的思路和代码，以期为其他初学者提供一点简单的思路。 点击查看 Source Code 本实例主要有以下四个步骤： 导入MNIST数据集。 定义网络模型。 模型训练。 模型测试。 环境准备本实例运行环境为：python3 + torch。需要导入的库如下所示。在运行本实例前，请确保以下库均安装成功。 123456789import osimport torchvision.datasets as datasetsimport torch.utils.datafrom torchvision import transformsimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variable 导入MNIST数据集MNIST（Mixed National Institute of Standards and Technology database）是一个计算机视觉数据集，它包含70000张手写数字的灰度图片，其中每一张图片包含 28*28 个像素点（如下图所示）。每一张图片都有对应的标签，也就是图片对应的数字。 数据集被分成两部分：60000 行的训练数据集（mnist.train）和10000行的测试数（mnist.test）。其中：60000 行的训练集分拆为 55000 行的训练集和 5000 行的验证集。 导入MNIST数据集的函数定义如下： 12345678910111213141516171819202122232425# Load training and test datadef load_data(path):​ train_data = datasets.MNIST(root=path,​ train=True,​ transform=transforms.Compose(​ [transforms.ToTensor(),​ transforms.Normalize((0.1307,), (0.3081,))]),​ target_transform=None,​ download=True)​ train = torch.utils.data.DataLoader(train_data,​ batch_size=64,​ shuffle=True,​ num_workers=0)​ test_data = datasets.MNIST(root=path,​ train=False,​ transform=transforms.Compose(​ [transforms.ToTensor(),​ transforms.Normalize((0.1307,), (0.3081,))]),​ target_transform=None,​ download=True)​ test = torch.utils.data.DataLoader(test_data,​ batch_size=64,​ shuffle=True,​ num_workers=0)​ return train, test 该函数传入参数为MNIST数据集的存放路径，输出分别为训练数据集和测试数据集。这里每次训练的图片数量batch_size选为64. 定义网络模型本实例中用到的网络模型由两个卷积层，两个池化层和两个全连接层组成。 第一层卷积层输入channel数为1，输出channel数选为10，卷积核大小为5*5。输入为64*1*28*28的张量，输出为64*10*24*24的张量。经过一个2*2的最大池化层，输出张量规模为64*10*12*12。 第二层卷积层输入channel数为10，输出channel数选为20，卷积核大小为5*5。输入为64*10*12*12的张量，输出为64*20*8*8的张量。经过一个2*2的最大池化层，输出张量规模为64*10*4*4。 经过两层卷积后，将所得张量经过两个全连接层，线性映射为1*10的张量，其中每个元素表示该张图片属于相应类别的概率。 网络模型的定义如下所示： 123456789101112131415161718192021222324252627class Net(nn.Module):​ def __init__(self):​ super(Net, self).__init__()​ self.conv1 = nn.Conv2d(in_channels=1,​ out_channels=10,​ kernel_size=5)​ self.conv2 = nn.Conv2d(in_channels=10,​ out_channels=20,​ kernel_size=5)​ self.conv2_drop = nn.Dropout2d()​ self.fc1 = nn.Linear(320, 50)​ self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.conv1(x) x = F.max_pool2d(x, kernel_size=2) x = F.relu(x) x = self.conv2(x) x = F.max_pool2d(x, kernel_size=2) x = F.relu(x) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=0) 模型训练1234567891011# Train the netdef train(model, epoch, train_loader, optimizer):​ model.train()​ for idx, (data, target) in enumerate(train_loader):​ optimizer.zero_grad()​ output = model(data)​ loss = F.nll_loss(output, target)​ loss.backward()​ optimizer.step()​ if idx % 50 == 49:​ print('Train epoch: %d Loss: %.3f ' % (epoch+1, loss)) 该函数输入参数为网络模型model，训练轮次epoch，训练数据集train_loader和优化方式optimizer。训练过程中损失函数使用负对数似然函数。 模型测试123456789# Test the netdef test(model, test_loader):​ model.eval()​ correct = 0​ for data, target in test_loader:​ output = model(data)​ predict = output.data.max(1)[1]​ correct = correct + predict.eq(target.data).sum()​ print('Accuracy: %2d' % (100*correct/10000), '%') 该函数传入参数为网络模型model，测试数据集test_loader，并将当前模型识别准确率打印在屏幕上。 到这里，整个实例已经全部定义完成，在主函数中依次调用相应的函数，即可实现手写体数字识别。 12345678910111213141516def main():​ data_base = './Datasets'​ mnist_path = os.path.join(data_base, 'MNIST')​ train_loader, test_loader = load_data(mnist_path) model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) epochs = 10 for epoch in range(epochs): train(model, epoch, train_loader, optimizer) test(model, test_loader)if __name__ == '__main__':​ main() 运行实例，笔者的测试准确率可以达到98%。 后记初学者刚接触机器学习，自身理解和认知有限，欢迎大家在评论区指正和评论。有任何问题也可以在评论区提出。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Pytorch</tag>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加评论系统Valine]]></title>
    <url>%2F2018%2F08%2F11%2FHexo%E6%B7%BB%E5%8A%A0%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9FValine%2F</url>
    <content type="text"><![CDATA[一款简洁，方便，好用的评论系统。 Step1：注册Leancloud 我们的评论系统是放在Leancloud上的,所以首先需要注册一个Leancloud账号。 点击进入Leancloud官网。 注册完成后需要先创建应用。点击创建应用，弹出如下界面： 应用名称可以随意取，笔者此处取名为Blog_comment，创建完成后单击进入应用。进入设置—应用Key ，可以看到APP ID 与 APP Key。 Step2：修改主题配置文件 打开主题配置文件 搜索 valine，填入appid 和 appkey。在对应位置填上步骤一中的APP ID 与 APP Key。 12345678910valine: enable: true appid: your appid appkey: your appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 保存后退出。在git bash 中执行: hexo server -p 2333在浏览器中输入 http://localhost:2333 ，可以看到添加评论系统后的博客。]]></content>
      <categories>
        <category>Hexo配置</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>评论</tag>
        <tag>Leancloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于BASYS2的音乐盒的制作与调试]]></title>
    <url>%2F2018%2F07%2F03%2F%E5%9F%BA%E4%BA%8EBASYS2%E7%9A%84%E9%9F%B3%E4%B9%90%E7%9B%92%E7%9A%84%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[摘要：基于BASYS2开发板，外接蜂鸣器，设计、调试并制作一个简易音乐盒。 关键词：BASYS2，Verilog，音乐发生器 实验原理 在基本掌握verilog语法和掌握BASYS2开发板的流程的基础上，通过自行设计、调试并制作一个简易音乐盒，进一步巩固自己所学的知识，掌握稍复杂电路的设计方法和制作流程，深化工程开发的体验，提高自身提出问题、分析问题和解决问题的能力。 实验要求 基础部分:制作一个简易音乐盒，将BASYS2开发板外接蜂鸣器，可以通过蜂鸣器播放出音乐。 拓展部分：在实现音乐播放的基础上，增加音乐暂停功能和切歌功能。 实验原理与设计音乐播放原理 音乐由音调和音长组成，其中。频率的高低决定了音调的高低，音符的持续时间和数目决定了音长。所以，只要将音调和音长控制好就能演奏出动听的乐曲。音乐播放的原理图如图所示。 音调控制 限于BASYS2开发板中只提供50Hz的时钟信号，所有不同频率的信号都是从只能从基准频率分频得来。因此需要选择合适的基准频率以及每个音符对应的分频比。由于分频比只能是整数，若基准频率过低，则分频比太小，四舍五入取整后的误差较大。若基准频率过高，虽然误差变小，但分频数将变大。实际的设计应综合考虑两方面的因素，在尽量减小频率误差的前提下选取合适的基准频率与每个音符的分频比。 通过查阅资料，发现基准频率一般选取6MHz。对应每个音符的频率和相应的分频比如下表所示。分频比是从6MHz基准频率通过二分频得到的3MHz基础上计算得到的。对于乐曲中的休止符，分频系数为0。 音调 低音 分频比 中音 分频比 高音 分频比 1 262 11450 523 5736 1046 2868 2 294 10204 587 5110 1175 2553 3 330 9090 659 4552 1318 2276 4 349 8595 698 4297 1397 2174 5 392 7653 784 3826 1568 1913 6 440 6818 880 3409 1760 1704 7 494 6072 988 3036 1967 1525 音长的控制 由于每首歌的曲速和节拍的时间有一定的差异，所以每个音符的持续时间会有差异。假定节奏较慢的音乐最短的音符为四分音符，全音符的持续时间为1s，则需要提供一个4Hz的时钟信号。若最短的音符为8分音符，则同理需要提供一个8Hz的时钟信号。当然，如果曲速不同导致全音符的时间不是1s时，需要调整相应的参数使的音乐不会有较大的失真。由于本实验中不要求对音乐的精准控制，所以该参数根据实际的播放效果调节即可。 音乐暂停键的实现 音乐暂停键的实现较为简单，可以由一个条件语句实现。将输出端与指定的暂停键关联，当暂停键为真时，将输出端从音乐时钟信号脱离，反之则将音乐时钟信号输出。该功能实现代码可以由问号-冒号运算符简单实现 音乐切换的实现 本实验中内置了两首曲目，并且是两首曲速不同的曲目。其中一首最短的音符为四分音符，另一首的最短的音符为八分音符。鉴于两手歌的曲速有较大的差异，因此需要两个乐谱时钟频率。可以通过程序定义乐谱时钟频率的选择与开发板上的一个button对应。当button被按下时，会产生一个时钟上升沿，检测到该上升沿后接通不同的时钟频率，并且更改播放曲目的编号，由此便实现了音乐切换的功能。 调试问题分析与解决曲速控制 曲速控制是一个比较麻烦的事情，需要不断调整分频比使的音乐基本不失真，并且，由于开发板上提供的时钟频率并不是严格的50MHz，所以对乐谱时钟频率的分频只能通过不断地尝试和3烧录后的结果来逐渐调整。尤其是两首歌的曲速不同需要不同的乐谱时钟频率，在确定该分频比的尝试中花费了不少时间。 当然，乐谱时钟频率也可以只使用一个，但是这样会导致曲速慢的曲目的音符大量重复，造成大量的冗余代码，所以两相权衡之下，还是选择了不同的乐谱时钟频率. 曲目切换 按照实际的应用场景来看，曲目切换通常会提供两个button，分别对应上一首和下一首，由于本实验中只提供了两首内置歌曲，所以只提供了一个切换键。 当然，在实现本功能的过程中，也遇到了一个问题。即开始的时候通过检测button的状态为0或是为1来判定是否切换歌曲。但是后来发现每次button按下的时候，有时候歌曲切换但有时候又不切换，经过一定的debug之后，猛然反应过来每次button被按下时，在button状态为1时，可能里面有很多个时钟，所以切歌状态瞬间发生了很多次，最终导致歌曲切换发生问题。 解决办法也比较简单，即不检测button的0/1状态，而是检测button的上升沿，这样便解决了该问题。 实验总结 通过本次实验，自己对verilog语言的特点和编程逻辑有了更深刻的认识。对BASYS2开发板的使用愈加娴熟。并且通过自行设计、调试并制作一个简易音乐盒，进一步巩固了自己所学的知识，掌握稍复杂电路的设计方法和制作流程，提高自身提出问题、分析问题和解决问题的能力。]]></content>
      <categories>
        <category>FPGA</category>
      </categories>
      <tags>
        <tag>FPGA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F02%2FMy%20First%20Blog%2F</url>
    <content type="text"><![CDATA[Hello，这是我的第一篇博客。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>爱狗</tag>
      </tags>
  </entry>
</search>
